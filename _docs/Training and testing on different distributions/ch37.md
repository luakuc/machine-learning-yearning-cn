---
title: 如何决定是否使用你所有的数据
permalink: /docs/ch37/
---

假设你的猫咪检测器的训练集包括 10000 张用户上传的图片，这些数据来自相同的数据分布且将作为单独的开发/测试集，同时也代表着你关心的将要处理的数据分布。你还从互联网下载了额外的 20000 张图片。此时你是否应该为你的学习算法提供所有的 20000 + 10000 张图片作为它的训练集，或者丢弃这 20000 张网络图片，以免它会影响你的学习算法呢？

在使用早期的学习算法（比如人为设计的计算机视觉特征，然后使用一个简单的线性分类器）时，真正的风险在于：合并这两种类型的数据会导致算法的表现更差。因此，一些工程师会警告你不要加入 20000 张互联网图片。 

但是有了现代强大而灵活的学习算法——比如大型的神经网络——这种风险已经大大降低了。如果你能够构建一个有足够多的隐藏单元/层的神经网络，你可以安全地将 20000 张图片添加到你的训练集。此时添加图片则更有可能提升算法的性能。

这种观察依赖于这样一个事实，即有一些 x-y 映射对于这两种类型的数据都很有效。换而言之，有这么些系统可以输入互联网图像或移动应用上的图像，并可靠地预测标签，即使它不知道图像的来源。

添加额外的 20000 张图片会产生以下影响：

1. 它给你的神经网络提供了更多关于猫咪外貌的样本。这是很有帮助的，因为互联网图片和用户上传的移动应用图片确实有一些相似之处。你的神经网络可以将从互联网图像中获得的一些知识应用到移动应用图像中。
2. 它迫使神经网络花费部分容量来学习网络图像的特定属性（比如更高的分辨率，不同画面结构图像的分布等等）。如果这些属性与移动应用图像有很大的不同，那么它将“耗尽”神经网络的一些表征能力，导致从移动应用图像的分布识别数据的能力就会降低，而这正是你真正关心的东西。从理论上讲，这可能会损害算法的性能。

换一种不同的术语来描述第二个影响，我们可以求助于小说中的人物夏洛克福尔摩斯，他解释道大脑就像一个阁楼；它只有有限的空间。他说，“每增加一个知识，你就会忘记你以前知道的东西。”因此，最重要的是，不要让无用的事实把有用的真相排挤出去。” （来自阿瑟柯南道尔的《血字的研究》 ）

幸运的是，如果你有足够的计算能力来构建一个足够大的神经网络——也就是一个足够大的阁楼——那么这就不是一个严重的问题了。你有足够的能力从互联网和移动应用图像中学习，而不会存在两种类型的数据在容量上的竞争。也即是说，你的算法的“大脑”足够大，不必担心会耗尽阁楼的空间。 

但是，如果你没有足够大的神经网络（或者另一个高度灵活的学习算法），那么你应该更加关注训练数据，需要与开发集/测试集的分布相匹配。

如果你认为有些数据没有任何帮助，那么出于计算原因应该将这些数据排除在训练集之外。例如，假设你的开发/测试集主要包含一些内容是人员、地点、地标、动物的任意图片。同时假设里面有大量的历史文档扫描图片,这些文件不包含任何类似猫的东西。它们看起来和开发/测试集的分布完全不同。没有必要将这些数据作为负样本，因为上述第一个影响带来的好处在这种情况下几乎忽略不计——你的神经网络几乎没有任何东西可以从这些数据中学习，但它们可以应用到开发/测试集中，加入它们将会浪费计算资源和神经网络的表征能力。 
